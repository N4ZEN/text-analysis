{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text analytics, also known as text mining, is a process of extracting value from large quantities of unstructured text data. While the text itself is structured to make sense to a human being (i.e. A company report split into sensible sections) it is unstructured from an analytics perspective because it doesn’t fit neatly into a relational database or rows and columns of a spreadsheet. Traditionally, the only structured part of text was the name of the document, the date it was created and who created it. Text analytics is particularly useful for information retrieval, pattern recognition, tagging and annotation, information extraction, sentiment assessment and predictive analytics. It could, for example, shed light on what your customers think of your product or service, or highlight the most common issues that your customers complain about.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = open('Data_1.txt')\n",
    "data1 = file.read()\n",
    "file.close()\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text analytics, also known as text mining, is a process of extracting value from large quantities of unstructured text data.', 'While the text itself is structured to make sense to a human being (i.e.', 'A company report split into sensible sections) it is unstructured from an analytics perspective because it doesn’t fit neatly into a relational database or rows and columns of a spreadsheet.', 'Traditionally, the only structured part of text was the name of the document, the date it was created and who created it.', 'Text analytics is particularly useful for information retrieval, pattern recognition, tagging and annotation, information extraction, sentiment assessment and predictive analytics.', 'It could, for example, shed light on what your customers think of your product or service, or highlight the most common issues that your customers complain about.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentence_tokens = sent_tokenize(data1)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'analytics,', 'also', 'known', 'as', 'text', 'mining,', 'is', 'a', 'process', 'of', 'extracting', 'value', 'from', 'large', 'quantities', 'of', 'unstructured', 'text', 'data.', 'While', 'the', 'text', 'itself', 'is', 'structured', 'to', 'make', 'sense', 'to', 'a', 'human', 'being', '(i.e.', 'A', 'company', 'report', 'split', 'into', 'sensible', 'sections)', 'it', 'is', 'unstructured', 'from', 'an', 'analytics', 'perspective', 'because', 'it', 'doesn’t', 'fit', 'neatly', 'into', 'a', 'relational', 'database', 'or', 'rows', 'and', 'columns', 'of', 'a', 'spreadsheet.', 'Traditionally,', 'the', 'only', 'structured', 'part', 'of', 'text', 'was', 'the', 'name', 'of', 'the', 'document,', 'the', 'date', 'it', 'was', 'created', 'and', 'who', 'created', 'it.', 'Text', 'analytics', 'is', 'particularly', 'useful', 'for', 'information', 'retrieval,', 'pattern', 'recognition,', 'tagging', 'and', 'annotation,', 'information', 'extraction,', 'sentiment', 'assessment', 'and', 'predictive', 'analytics.', 'It', 'could,', 'for', 'example,', 'shed', 'light', 'on', 'what', 'your', 'customers', 'think', 'of', 'your', 'product', 'or', 'service,', 'or', 'highlight', 'the', 'most', 'common', 'issues', 'that', 'your', 'customers', 'complain', 'about.\\n']\n"
     ]
    }
   ],
   "source": [
    "split_tokens = data1.split(\" \")\n",
    "print(split_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'analytics', ',', 'also', 'known', 'as', 'text', 'mining', ',', 'is', 'a', 'process', 'of', 'extracting', 'value', 'from', 'large', 'quantities', 'of', 'unstructured', 'text', 'data', '.', 'While', 'the', 'text', 'itself', 'is', 'structured', 'to', 'make', 'sense', 'to', 'a', 'human', 'being', '(', 'i.e', '.', 'A', 'company', 'report', 'split', 'into', 'sensible', 'sections', ')', 'it', 'is', 'unstructured', 'from', 'an', 'analytics', 'perspective', 'because', 'it', 'doesn', '’', 't', 'fit', 'neatly', 'into', 'a', 'relational', 'database', 'or', 'rows', 'and', 'columns', 'of', 'a', 'spreadsheet', '.', 'Traditionally', ',', 'the', 'only', 'structured', 'part', 'of', 'text', 'was', 'the', 'name', 'of', 'the', 'document', ',', 'the', 'date', 'it', 'was', 'created', 'and', 'who', 'created', 'it', '.', 'Text', 'analytics', 'is', 'particularly', 'useful', 'for', 'information', 'retrieval', ',', 'pattern', 'recognition', ',', 'tagging', 'and', 'annotation', ',', 'information', 'extraction', ',', 'sentiment', 'assessment', 'and', 'predictive', 'analytics', '.', 'It', 'could', ',', 'for', 'example', ',', 'shed', 'light', 'on', 'what', 'your', 'customers', 'think', 'of', 'your', 'product', 'or', 'service', ',', 'or', 'highlight', 'the', 'most', 'common', 'issues', 'that', 'your', 'customers', 'complain', 'about', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk_tokens= word_tokenize(data1)\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'analytic', ',', 'also', 'known', 'a', 'text', 'min', ',', 'i', 'a', 'proces', 'of', 'extract', 'value', 'from', 'large', 'quantit', 'of', 'unstructur', 'text', 'data', '.', 'While', 'the', 'text', 'itself', 'i', 'structur', 'to', 'make', 'sense', 'to', 'a', 'human', 'be', '(', 'i.e', '.', 'A', 'company', 'report', 'split', 'into', 'sensible', 'section', ')', 'it', 'i', 'unstructur', 'from', 'an', 'analytic', 'perspect', 'because', 'it', 'doesn', '’', 't', 'fit', 'neat', 'into', 'a', 'relational', 'database', 'or', 'row', 'and', 'column', 'of', 'a', 'spreadsheet', '.', 'Traditional', ',', 'the', 'on', 'structur', 'part', 'of', 'text', 'wa', 'the', 'name', 'of', 'the', 'docu', ',', 'the', 'date', 'it', 'wa', 'creat', 'and', 'who', 'creat', 'it', '.', 'Text', 'analytic', 'i', 'particular', 'useful', 'for', 'information', 'retrieval', ',', 'pattern', 'recognition', ',', 'tagg', 'and', 'annotation', ',', 'information', 'extraction', ',', 'senti', 'assess', 'and', 'predict', 'analytic', '.', 'It', 'could', ',', 'for', 'example', ',', 'sh', 'light', 'on', 'what', 'your', 'customer', 'think', 'of', 'your', 'product', 'or', 'service', ',', 'or', 'highlight', 'the', 'most', 'common', 'issu', 'that', 'your', 'customer', 'complain', 'about', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem\n",
    "\n",
    "print([stem(t) for t in nltk_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'analyt', ',', 'also', 'known', 'as', 'text', 'mine', ',', 'is', 'a', 'process', 'of', 'extract', 'valu', 'from', 'larg', 'quantiti', 'of', 'unstructur', 'text', 'data', '.', 'while', 'the', 'text', 'itself', 'is', 'structur', 'to', 'make', 'sens', 'to', 'a', 'human', 'be', '(', 'i.e', '.', 'A', 'compani', 'report', 'split', 'into', 'sensibl', 'section', ')', 'it', 'is', 'unstructur', 'from', 'an', 'analyt', 'perspect', 'becaus', 'it', 'doesn', '’', 't', 'fit', 'neatli', 'into', 'a', 'relat', 'databas', 'or', 'row', 'and', 'column', 'of', 'a', 'spreadsheet', '.', 'tradit', ',', 'the', 'onli', 'structur', 'part', 'of', 'text', 'wa', 'the', 'name', 'of', 'the', 'document', ',', 'the', 'date', 'it', 'wa', 'creat', 'and', 'who', 'creat', 'it', '.', 'text', 'analyt', 'is', 'particularli', 'use', 'for', 'inform', 'retriev', ',', 'pattern', 'recognit', ',', 'tag', 'and', 'annot', ',', 'inform', 'extract', ',', 'sentiment', 'assess', 'and', 'predict', 'analyt', '.', 'It', 'could', ',', 'for', 'exampl', ',', 'shed', 'light', 'on', 'what', 'your', 'custom', 'think', 'of', 'your', 'product', 'or', 'servic', ',', 'or', 'highlight', 'the', 'most', 'common', 'issu', 'that', 'your', 'custom', 'complain', 'about', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()  \n",
    "\n",
    "print([porter.stem(w) for w in nltk_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'analys', ',', 'also', 'known', 'as', 'text', 'min', ',', 'is', 'a', 'process', 'of', 'extract', 'valu', 'from', 'larg', 'quant', 'of', 'unstruct', 'text', 'dat', '.', 'whil', 'the', 'text', 'itself', 'is', 'structured', 'to', 'mak', 'sens', 'to', 'a', 'hum', 'being', '(', 'i.e', '.', 'a', 'company', 'report', 'split', 'into', 'sens', 'sect', ')', 'it', 'is', 'unstruct', 'from', 'an', 'analys', 'perspect', 'becaus', 'it', 'doesn', '’', 't', 'fit', 'neat', 'into', 'a', 'rel', 'databas', 'or', 'row', 'and', 'column', 'of', 'a', 'spreadsheet', '.', 'tradit', ',', 'the', 'on', 'structured', 'part', 'of', 'text', 'was', 'the', 'nam', 'of', 'the', 'docu', ',', 'the', 'dat', 'it', 'was', 'cre', 'and', 'who', 'cre', 'it', '.', 'text', 'analys', 'is', 'particul', 'us', 'for', 'inform', 'retriev', ',', 'pattern', 'recognit', ',', 'tag', 'and', 'annot', ',', 'inform', 'extract', ',', 'senty', 'assess', 'and', 'predict', 'analys', '.', 'it', 'could', ',', 'for', 'exampl', ',', 'shed', 'light', 'on', 'what', 'yo', 'custom', 'think', 'of', 'yo', 'produc', 'or', 'serv', ',', 'or', 'highlight', 'the', 'most', 'common', 'issu', 'that', 'yo', 'custom', 'complain', 'about', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "print([lancaster.stem(t) for t in nltk_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  154 words in this text\n",
      "\n",
      "There are  96 words in this text after removing stop words. \n",
      "\n",
      "['Text', 'analytics', ',', 'also', 'known', 'text', 'mining', ',', 'process', 'extracting', 'value', 'large', 'quantities', 'unstructured', 'text', 'data', '.', 'While', 'text', 'structured', 'make', 'sense', 'human', '(', 'i.e', '.', 'A', 'company', 'report', 'split', 'sensible', 'sections', ')', 'unstructured', 'analytics', 'perspective', '’', 'fit', 'neatly', 'relational', 'database', 'rows', 'columns', 'spreadsheet', '.', 'Traditionally', ',', 'structured', 'part', 'text', 'name', 'document', ',', 'date', 'created', 'created', '.', 'Text', 'analytics', 'particularly', 'useful', 'information', 'retrieval', ',', 'pattern', 'recognition', ',', 'tagging', 'annotation', ',', 'information', 'extraction', ',', 'sentiment', 'assessment', 'predictive', 'analytics', '.', 'It', 'could', ',', 'example', ',', 'shed', 'light', 'customers', 'think', 'product', 'service', ',', 'highlight', 'common', 'issues', 'customers', 'complain', '.']\n",
      "\n",
      "The stopwords present are: \n",
      "['as', 'is', 'a', 'of', 'from', 'the', 'itself', 'to', 'being', 'into', 'it', 'an', 'because', 'doesn', 't', 'or', 'and', 'only', 'was', 'who', 'for', 'on', 'what', 'your', 'most', 'that', 'about']\n",
      "\n",
      "There are  77 words in this text after removing stop words and punctutuations. \n",
      "\n",
      "['Text', 'analytics', 'also', 'known', 'text', 'mining', 'process', 'extracting', 'value', 'large', 'quantities', 'unstructured', 'text', 'data', 'While', 'text', 'structured', 'make', 'sense', 'human', 'i.e', 'A', 'company', 'report', 'split', 'sensible', 'sections', 'unstructured', 'analytics', 'perspective', '’', 'fit', 'neatly', 'relational', 'database', 'rows', 'columns', 'spreadsheet', 'Traditionally', 'structured', 'part', 'text', 'name', 'document', 'date', 'created', 'created', 'Text', 'analytics', 'particularly', 'useful', 'information', 'retrieval', 'pattern', 'recognition', 'tagging', 'annotation', 'information', 'extraction', 'sentiment', 'assessment', 'predictive', 'analytics', 'It', 'could', 'example', 'shed', 'light', 'customers', 'think', 'product', 'service', 'highlight', 'common', 'issues', 'customers', 'complain']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "#number of words initially. \n",
    "print(\"There are \", len(nltk_tokens), \"words in this text\\n\")\n",
    "\n",
    "# removes stop words from text and also creates another list with unique stop words \n",
    "stopTokens = stopwords.words(\"english\") \n",
    "filteredTokens = []\n",
    "stopWordss = []\n",
    "for w in nltk_tokens:\n",
    "    if w not in stopTokens :\n",
    "        filteredTokens.append(w)\n",
    "    elif w in stopTokens :\n",
    "        if w not in stopWordss:\n",
    "            stopWordss.append(w)\n",
    "\n",
    "        #number of words after removing stop words\n",
    "print(\"There are \", len(filteredTokens), \"words in this text after removing stop words. \\n\")\n",
    "print(filteredTokens)\n",
    "\n",
    "#printing the stopwords present\n",
    "print('\\nThe stopwords present are: ')\n",
    "print(stopWordss)\n",
    "\n",
    "#removes punctutaion \n",
    "punctuatnTokens = list(string.punctuation)\n",
    "filtereTokens = []\n",
    "for w in filteredTokens:\n",
    "    if w not in punctuatnTokens :\n",
    "        filtereTokens.append(w)\n",
    "        \n",
    "          #number of words after removing stop words and punctuations\n",
    "print(\"\\nThere are \", len(filtereTokens), \"words in this text after removing stop words and punctutuations. \\n\")\n",
    "print(filtereTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP is important for scientific, economic, social, and cultural reasons. NLP is experiencing rapid growth as its theories and methods are deployed in a variety of new language technologies. \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "file = open('C://Users//Naza Zuhair//Downloads//Sem 5//TXSA//assignment//Individual Assignment Data//Data_2.txt')\n",
    "data2 = file.read()\n",
    "file.close()\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLP', 'NN'), ('is', 'NNS'), ('important', 'NN'), ('for', 'NN'), ('scientific', 'NN'), (',', 'NN'), ('economic', 'NN'), (',', 'NN'), ('social', 'NN'), (',', 'NN'), ('and', 'NN'), ('cultural', 'NN'), ('reasons', 'NNS'), ('.', 'NN'), ('NLP', 'NN'), ('is', 'NNS'), ('experiencing', 'VBG'), ('rapid', 'NN'), ('growth', 'NN'), ('as', 'NNS'), ('its', 'NNS'), ('theories', 'VBZ'), ('and', 'NN'), ('methods', 'NNS'), ('are', 'NN'), ('deployed', 'VBD'), ('in', 'NN'), ('a', 'NN'), ('variety', 'NN'), ('of', 'NN'), ('new', 'NN'), ('language', 'NN'), ('technologies', 'VBZ'), ('.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "patterns = [\n",
    "     (r'.*ing$', 'VBG'),               # gerunds\n",
    "     (r'.*ed$', 'VBD'),                # simple past\n",
    "     (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "     (r'.*ould$', 'MD'),               # modals\n",
    "     (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "     (r'.*s$', 'NNS'),                 # plural nouns\n",
    "     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "     (r'.*', 'NN'),                    # nouns (default)\n",
    "     (r'^\\d+$', 'CD'),\n",
    "     (r'.*ing$', 'VBG'),               # gerunds, i.e. wondering\n",
    "     (r'.*ment$', 'NN'),               # i.e. wonderment\n",
    "     (r'.*ful$', 'JJ')                 # i.e. wonderful\n",
    " ]\n",
    "\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "tagger=nltk.tag.sequential.RegexpTagger(patterns)\n",
    "print(tagger.tag(tokenized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLP', 'NNP'), ('is', 'VBZ'), ('important', 'JJ'), ('for', 'IN'), ('scientific', 'JJ'), ('economic', 'JJ'), ('social', 'JJ'), ('and', 'CC'), ('cultural', 'JJ'), ('reasons', 'NNS'), ('NLP', 'NNP'), ('is', 'VBZ'), ('experiencing', 'VBG'), ('rapid', 'JJ'), ('growth', 'NN'), ('as', 'IN'), ('its', 'PRP$'), ('theories', 'NNS'), ('and', 'CC'), ('methods', 'NNS'), ('are', 'VBP'), ('deployed', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('variety', 'NN'), ('of', 'IN'), ('new', 'JJ'), ('language', 'NN'), ('technologies', 'NNS')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob\n",
    "final = TextBlob(data2)\n",
    "print(final.tags)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLP', 'NNP'), ('is', 'VBZ'), ('important', 'JJ'), ('for', 'IN'), ('scientific', 'JJ'), (',', ','), ('economic', 'JJ'), (',', ','), ('social', 'JJ'), (',', ','), ('and', 'CC'), ('cultural', 'JJ'), ('reasons', 'NNS'), ('.', '.'), ('NLP', 'NNP'), ('is', 'VBZ'), ('experiencing', 'VBG'), ('rapid', 'JJ'), ('growth', 'NN'), ('as', 'IN'), ('its', 'PRP$'), ('theories', 'NNS'), ('and', 'CC'), ('methods', 'NNS'), ('are', 'VBP'), ('deployed', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('variety', 'NN'), ('of', 'IN'), ('new', 'JJ'), ('language', 'NN'), ('technologies', 'NNS'), ('.', '.')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(nltk.pos_tag(tokenized))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n"
     ]
    }
   ],
   "source": [
    "#to get the meaning of the tags\n",
    "nltk.help.upenn_tagset('VBD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
